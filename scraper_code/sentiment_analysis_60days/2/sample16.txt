NTSB Hearing Blames Humans, Software And Policy For Fatal Uber Robocar Crash - But Mostly Humans performance.mark = performance.mark || function(s) {}; performance.mark('content_title_end'); Brad Templeton Contributor Opinions expressed by Forbes Contributors are their own. Transportation I cover robocar technology & previously worked on Google's car team. Share to facebook Share to twitter Share to linkedin Robert Sumwalt, Chairman of the NTSB, convenes session on Uber crashNTSBThe National Transportation Safety Board presented its findings today on the fatal crash involving an Uber test robocar and Elaine Herzberg. Not unexpectedly, they placed blame on the safety culture at Uber, the safety drivers watching of a streaming video show, and to some degree, on the impaired state of the pedestrian.Most notably, a correction is required on earlier reports that the Uber system could not identify pedestrians outside of crosswalks. That was not the case, and while there were flaws related to that issue, they played no role in this accident.The inappropriate actions of both the automatic driving system as implemented and the vehicle's human operator were symptoms of a deeper problem. The ineffective safety culture that existed at the time of Uber ATG Robert Sumwalt, NTSB chairmanI wrote extensive coverage earlier on the report. The hearing revealed a few extra snippets of detail not highlighted in the written report.Probable Cause ReportThe NTSBs final determination of probable cause put primary blame on the safety drivers inattention. Contributory causes where Ubers lack of safety culture, poor monitoring of safety drivers, and lack of countermeasures for automation complacency. They put tertiary blame on the pedestrians impaired crossing of the road, and the lack of good regulations at the Arizona and Federal levels. Most notably, they do not attribute the technology failures as causes of the crash. This is a correct cause ruling all tested vehicles, while generally better than Ubers, have flaws which would lead to a crash with a negligent safety driver, and to blame those flaws would be to blame the idea of testing this way at all.Human errorsWhen it comes to human fault, the report noted that Herzberg had a high concentration of methamphetamine (more than 10 times the medicinal dose) in her blood which would alter her perception. She also had some marijuana residue. She did not look to her right at the oncoming vehicle until 1 second before the crash.There was also confirmation that the safety driver had indeed pulled out a cell phone and was streaming a TV show on it, looking down at it 34% of the time during her driving session, with a full 5 second glance from 6 to 1 seconds prior to the impact.While Uber recorded videos of safety drivers, they never reviewed those of this driver to learn that she was violating the policy against cell phone use. She had received no reprimands, and driven this stretch of road 73 times before.While there is also a small tablet computer in each car for the operator, it is very simple, and just shows the navigation route like other navigation computers, and had very few inputs. NTSB attributed none of the distraction to this tablet. Uber claims it designed the tablet UX in accordance with NHTSA guidelines for in-car driver UX. Some teams use a voice system rather than a touch system to avoid distraction when reporting anomalies.Machine errorsMore detail was revealed about how Ubers system dealt with pedestrians. Earlier reports (including my own) stated that the system was unable to classify an obstacle as a pedestrian outside of crosswalks. This is not true, though the truth is somewhat more confusing. It simply never classified her as a pedestrian, probably because she was walking a bicycle. What confused writers about the accident was a mention of potential failure (which did not actually occur) in the systems goal-assignment modules.When a robocar system identifies an obstacle, it tries to predict where it may move, and to do so estimates a goal for it. For example, a pedestrian in a crosswalk will probably have the goal of crossing the road. A car or bicycle in a lane will have the likely goal of continuing to drive down that lane (with some chance of changing lanes.)A flaw which played no role in this crash was that Ubers system would not assign crossing the road as a goal to a pedestrian not in a crosswalk. If they had identified her has a pedestrian then they would have not viewed it as likely she was crossing the road.Instead, she was classified as things like a vehicle, cyclist or unknown obstacle. The goal guesses for those objects are different, and in many cases are based on the past history of movement of the object. Thats why the flaw of forgetting the past history after reclassification was the real technical error here. Each time it reclassified her, it had to predict her motion with no past history, and thus it was not able to understand that, whatever she was, she was crossing the road and going to enter their lane. This lack of object persistence remains the greater error than the classification errors. Classification errors happen all the time in robocar systems, but that is supposed to be mitigated by the fact that the systems track and object as it moves without knowing exactly what it is. Uber failed to do that.More detail was revealed about the disabling of the Volvo standard automatic emergency braking system that comes with the SUV. The Volvo system had its own radar on the same frequency as Ubers radar and thus could not be used at the same time. Later, they were able to re-tune one of the radars so that both systems can be active.Automation ComplacencySome focus was placed on the issue of how well humans do at monitoring automated systems, and the tendency for people to become complacent and not diligent. This is a common factor in transportation modes that have some amount of automation. Uber never reviewed videos of the safety driver (and rarely viewed those of others) but now, has a third party do spot-checks on safety drivers to look for problems like this. Now Uber, and other companies use automated systems that track driver gaze to make sure eyes are kept on the road and even alert if they are off the road too much. Uber has returned to having two safety drivers.An interesting statistic was revealed, that Uber had 40 test vehicles in Tempe and 254 operators on staff. This makes it baffling that they wanted to switch to having one safety driver instead of two. They seem to have had more than enough staff to keep two in each vehicle.While generally two safety drivers are better than one, and would prevent an accident of this sort, I now believe that computerized monitoring of one safety driver is actually probably enough. The record of Tesla autopilot drivers, who are not trained and usually drive alone, suggests that having a single person doing oversight of a decent automated system results in an adequate safety record. A trained safety driver with automated monitoring should do even better.Policy issuesUber ATGs poor safety culture or in some cases, lack of safety culture, saw much criticism, as has been reported before. According to the report by Robert Fox, they did not have a proper framework for risk mitigation, they had bad policies and procedures, and bad oversight of vehicle operators. Uber, it is reported, has fixed many of these things, and at present is only doing very limited testing just a one mile loop around their HQ limited to 25mph.Critique of Uber pre-crash policiesNTSBEnzar Becic of NTSB reported on the fairly basic rules currently applied to companies testing robocars, particularly in Arizona, which puts no requirements on teams testing with safety drivers. The NHTSA regulations request an optional safety assessment, but only 16 companies have filed one (over 64 companies are registered in California.)Chairman Sumwalt praised Uber for its cooperation with their investigation, and made some strong digs against Tesla for the way it had not helped and had to be forced out of one of their investigations. Sumwalt said he liked that Ubers CEO did not hang up on me, strongly implying that Elon Musk might have hung up on him during conversations.More notes on the technical faultsIn my earlier report on the Uber fatality, I included a lot of discussion on two new technical details, namely (incorrectly) the Uber systems configuration to not identify pedestrians who were in the middle of the road outside of crosswalks, and more importantly, the flaw in their object tracking system which had it forget any past learnings every time it reclassified an object. Many other press also made reports on these issues, most of them putting a strong focus on the problem with the identification of pedestrians.Even though I made it very clear that these technical problems, while a sign of poor system design and coding, were not the cause of the accident, it has been natural for readers to take it that way. As such it is important to reiterate that all robocar teams, to my knowledge, have issues where there system does not properly classify things in its view, and which have made it necessary, from time to time, for a safety driver to take over to prevent an accident. With brand new projects, this can be a very frequent event, and there is nothing wrong with that, as long as a trained and attentive safety driver is there to do just that. While we can and should be critical of the poor quality of Ubers software, I doubt any project out there hasnt gone through a very early period where their system was immature and prone to problems like this, which were handled correctly by a safety driver. If any of these teams had deployed one of their early cars with a safety driver who sat watching a video, as is alleged here, those cars probably would have gotten into an accident, and possibly a fatal one if circumstances were wrong.It does not excuse Uber, but they also had some bad luck here. Normally, a pedestrian crossing a high speed street outside a crosswalk would exercise some minimal caution, starting with look both ways before crossing the street as we are all taught as children. By all appearances, the crash took place late on a Sunday night on a largely empty road, exactly the sort of situation where a person would normally hear any approaching car well in advance, and check regularly to the right for oncoming traffic, which would be very obvious because of its headlights obvious even in peripheral vision. Herzberg crossed obliviously, looking over just one second before impact. NTSB investigators attributed this to the meth in her system. They did not know if the concentration in her blood was going up (due to recently taken doses) and altering perception, or coming down (causing unusual moods.)It was also reported that pedestrian crossings at this location are rare (though were more common that night due to a concert) and there is no recollection of another pedestrian incident in this location.This does not mean that robocars and safety drivers need not be be ready for oblivious pedestrians outside of crosswalks, of course. These exist, even if most pedestrians are more prudent. In fact, 71% of pedestrian hits in Arizona took place outside of intersections.What about the cameras?Questions remain about the role of computer vision and cameras in the Uber fatality. The NTSB report says almost nothing about the camera systems in the car, other than to note that the close-range cameras, which would not have come into play in this incident, were not in use.Most self-driving car designs make use of 3 different systems to detect something like a pedestrian on the road ahead. The LIDAR is the most reliable, and is used by all major players except Tesla. The radar is also very good but does not work as well on a pedestrian who is not moving towards you or away from you. In this case, Herzberg was crossing the street and thus moving perpendicular to the car. Radar did still see her but provides much less information. Because Ubers software kept reclassifying her and forgetting what it learned before, and radar is not good at providing a detailed horizontal position, it was not so useful for noticing that she was walking across the road.Cameras should have played a useful role here. While computer vision doesnt have the 100% certain knowledge of distance you get from LIDAR and radar, it can be quite good at figuring out what something is, and all teams train their systems to identify pedestrians, and one presumes, people walking bicycles. Ubers system never did that. But while its hard for a LIDAR to identify something like a pedestrian walking a bicycle as specifically that at a long distance its just a diffuse blob of dots until you get close this is something cameras should have had a better shot at. Yet, the NTSB report discusses nothing about what the vision perception system saw.Was it off? Is it simply not very functional? Like all the other technical issues, they take second place to the issue of bad safety driving, but they are still interesting for all of us who want to understand what sort of mistakes programmers can make and how to avoid them.One issue with vision systems is how they operate at night. An ordinary camera will not simultaneously get good images of both bright things (such as car headlamps and things lit by streetlamps and headlamps) and things in the shadow. Herzberg went in and out of pools of light. The frequently shown dashcam video of the crash reveals the poor performance of an ordinary camera. Thats why robocar teams have been actively developing high dynamic range camera approaches to improve vision results. One way to do that is to have 2 or more cameras, each one set at a different exposure level to get a good image of the shadows and the well lit areas. A cheaper way to do that is just have one camera that alternates between taking bright and dark exposures every other frame. These techniques should have allowed the many cameras on the Uber car to get a good image of the road and Herzberg even though it was night.Slide decks and video are available on the NTSB site.Board member Jennifer Homendy was highly critical of the (lack of) regulation around testing, and feels there should be more federal and state regulations on the testing of these vehicles. She expressed concern over the NHTSA regulations which are currently fairly minimal. They are minimal because NHTSA and the states have realized they do not have the skills yet to regulate a technology which is still in development and is constantly changing. Nonetheless, NTSB believes NHTSA should make submission of the safety plan reports be mandatory.Diagram of UX for Uber's operator tabletNTSBReports FindingsHere are the final findings (from the final report)1. None of the following were factors in the crash: (1) driver licensing, experience, or knowledge of the automated driving system operation; (2) vehicle operator substance impairment or fatigue; or (3) mechanical condition of the vehicle. 2. The emergency response to the crash was timely and adequate. 3. The pedestrians unsafe behavior in crossing the street in front of the approaching vehicle at night and at a location without a crosswalk violated Arizona statutes and was possibly due to diminished perception and judgment resulting from drug use. 4. The Uber Advanced Technologies Group did not adequately manage the anticipated safety risk of its automated driving systems functional limitations, including the systems inability in this crash to correctly classify and predict the path of the pedestrian crossing the road midblock. 5. The aspect of the automated driving systems design that precluded braking in emergency situations only when a crash was unavoidable increased the safety risks associated with testing automated driving systems on public roads. 6. Because the Uber Advanced Technologies Groups automated driving system was developmental, with associated limitations and expectations of failure, the extent to which those limitations pose a safety risk depends on safety redundancies and 3 mitigation strategies designed to reduce the safety risk associated with testing automated driving systems on public roads. 7. The Uber Advanced Technologies Groups deactivation of the Volvo forward collision warning and automatic emergency braking systems without replacing their full capabilities removed a layer of safety redundancy and increased the risks associated with testing automated driving systems on public roads. 8. Postcrash changes by the Uber Advanced Technologies Group, such as making Volvos forward collision warning and automatic emergency braking available during operation of the automated driving system (ADS), added a layer of safety redundancy that reduces the safety risks associated with testing ADSs on public roads. 9. Had the vehicle operator been attentive, she would likely have had sufficient time to detect and react to the crossing pedestrian to avoid the crash or mitigate the impact. 10. The vehicle operators prolonged visual distraction, a typical effect of automation complacency, led to her failure to detect the pedestrian in time to avoid the collision. 11. The Uber Advanced Technologies Group did not adequately recognize the risk of automation complacency and develop effective countermeasures to control the risk of vehicle operator disengagement, which contributed to the crash. 12. Although the installation of a human-machine interface in the Uber Advanced Technologies Group test vehicles reduced the complexity of the automation-monitoring task, the decision to remove the second vehicle operator increased the task demands on the sole operator and also reduced the safety redundancies that would have minimized the risks associated with testing automated driving systems on public roads. 13. Although the Uber Advanced Technologies Group had the means to retroactively monitor the behavior of vehicle operators and their adherence to operational procedures, it rarely did so; and the detrimental effect of the companys ineffective oversight was exacerbated by its decision to remove the second vehicle operator during testing of the automated driving system. 14. The Uber Advanced Technologies Groups postcrash inclusion of a second vehicle operator during testing of the automated driving system, along with real-time monitoring of operator attentiveness, begins to address the oversight deficiencies that contributed to the crash. 15. The Uber Advanced Technologies Groups inadequate safety culture created conditionsincluding inadequate oversight of vehicle operatorsthat contributed to the circumstances of the crash and specifically to the vehicle operators extended distraction during the crash trip. 16. The Uber Advanced Technologies Groups plan for implementing a safety management system, as well as postcrash changes in the companys oversight of vehicle operators, begins to address the deficiencies in safety risk management that contributed to the crash. 17. Mandatory submission of safety self-assessment reportswhich are currently voluntaryand their evaluation by the National Highway Traffic Safety 4 Administration would provide a uniform, minimal level of assessment that could aid states with legislation pertaining to the testing of automated vehicles. 18. Arizonas lack of a safety-focused application-approval process for automated driving system (ADS) testing at the time of the crash, and its inaction in developing such a process since the crash, demonstrate the states shortcomings in improving the safety of ADS testing and safeguarding the public. 19. Considering the lack of federal safety standards and assessment protocols for automated driving systems, as well as the National Highway Traffic Safety Administrations inadequate safety self-assessment process, states that have no, or only minimal, requirements related to automated vehicle testing can improve the safety of such testing by implementing a thorough application and review process before granting testing permits.You can read/leave comments at this site. Follow me on Twitter or LinkedIn. Check out my website. Brad Templeton I founded ClariNet, the world's first internet based business,am Chairman Emeritus of the Electronic Frontier Foundation, and a director of the Foresight Institut... Read More Print Site Feedback Tips Corrections Reprints & Permissions Terms Privacy 2019 Forbes Media LLC. All Rights Reserved. AdChoices

